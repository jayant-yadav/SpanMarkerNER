{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-level context\n",
    "[SpanMarker](https://github.com/tomaarsen/SpanMarkerNER) is an accessible yet powerful Python module for training Named Entity Recognition models.\n",
    "\n",
    "In this tutorial, I'll show you how to perform training and inference of SpanMarker models using document-level context to improve performance.\n",
    "\n",
    "Many approaches to NER process individual sentences completely independently of another, even if the sentences originate from the same document. Although this works fine, research has shown that including additional contextual information (i.e. the previous and next sentence(s)) improves the performance of the model. In my own experiments of SpanMarker with CoNLL03, including this document-level contextual information improves the model from a mean F1 of 92.9±0.0 to a mean F1 of 94.1±0.1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-level context in SpanMarker\n",
    "SpanMarker is designed to require only slight changes in the input data to allow for document-level context during training, evaluating and inference. In particular, the only required change is that the input must now be a [Dataset](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset) with `document_id` and `sentence_id` columns. \n",
    "\n",
    "#### Training and evaluating\n",
    "For training and evaluation, the dataset must now contain `tokens`, `ner_tags`, `document_id` and `sentence_id` columns. I've prepared two datasets ([tomaarsen/conll2003](https://huggingface.co/datasets/tomaarsen/conll2003), [tomaarsen/conllpp](https://huggingface.co/datasets/tomaarsen/conllpp)) that I've used to train some models. We will have a look at the former to get a feel for how these values are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document_id', 'sentence_id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 14041\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load the dataset from the Hub and throw away the non-NER columns\n",
    "dataset = load_dataset(\"tomaarsen/conll2003\", split=\"train\").remove_columns((\"id\", \"chunk_tags\", \"pos_tags\"))\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[5, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[Germany, 's, representative, to, the, Europea...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[\", We, do, n't, support, any, such, recommend...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[He, said, further, scientific, study, was, re...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[He, said, a, proposal, last, month, by, EU, F...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[Fischler, proposed, EU-wide, measures, after,...</td>\n",
       "      <td>[1, 0, 7, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>[But, Fischler, agreed, to, review, his, propo...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[Spanish, Farm, Minister, Loyola, de, Palacio,...</td>\n",
       "      <td>[7, 0, 0, 1, 2, 2, 0, 0, 0, 1, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[.]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>[Only, France, and, Britain, backed, Fischler,...</td>\n",
       "      <td>[0, 5, 0, 5, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>[The, EU, 's, scientific, veterinary, and, mul...</td>\n",
       "      <td>[0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>[Sheep, have, long, been, known, to, contract,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>[British, farmers, denied, on, Thursday, there...</td>\n",
       "      <td>[7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>[\", What, we, have, to, be, extremely, careful...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>[Bonn, has, led, efforts, to, protect, public,...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>[Germany, imported, 47,600, sheep, from, Brita...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>[It, brought, in, 4,275, tonnes, of, British, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Rare, Hendrix, song, draft, sells, for, almos...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[LONDON, 1996-08-22]</td>\n",
       "      <td>[5, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, rare, early, handwritten, draft, of, a, so...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 2, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[A, Florida, restaurant, paid, 10,925, pounds,...</td>\n",
       "      <td>[0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[At, the, end, of, a, January, 1967, concert, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 5, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[Buyers, also, snapped, up, 16, other, items, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[They, included, a, black, lacquer, and, mothe...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>[The, guitarist, died, of, a, drugs, overdose,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[China, says, Taiwan, spoils, atmosphere, for,...</td>\n",
       "      <td>[5, 0, 5, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[BEIJING, 1996-08-22]</td>\n",
       "      <td>[5, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_id  sentence_id  \\\n",
       "0             1            0   \n",
       "1             1            1   \n",
       "2             1            2   \n",
       "3             1            3   \n",
       "4             1            4   \n",
       "5             1            5   \n",
       "6             1            6   \n",
       "7             1            7   \n",
       "8             1            8   \n",
       "9             1            9   \n",
       "10            1           10   \n",
       "11            1           11   \n",
       "12            1           12   \n",
       "13            1           13   \n",
       "14            1           14   \n",
       "15            1           15   \n",
       "16            1           16   \n",
       "17            1           17   \n",
       "18            1           18   \n",
       "19            1           19   \n",
       "20            2            0   \n",
       "21            2            1   \n",
       "22            2            2   \n",
       "23            2            3   \n",
       "24            2            4   \n",
       "25            2            5   \n",
       "26            2            6   \n",
       "27            2            7   \n",
       "28            3            0   \n",
       "29            3            1   \n",
       "\n",
       "                                               tokens  \\\n",
       "0   [EU, rejects, German, call, to, boycott, Briti...   \n",
       "1                                  [Peter, Blackburn]   \n",
       "2                              [BRUSSELS, 1996-08-22]   \n",
       "3   [The, European, Commission, said, on, Thursday...   \n",
       "4   [Germany, 's, representative, to, the, Europea...   \n",
       "5   [\", We, do, n't, support, any, such, recommend...   \n",
       "6   [He, said, further, scientific, study, was, re...   \n",
       "7   [He, said, a, proposal, last, month, by, EU, F...   \n",
       "8   [Fischler, proposed, EU-wide, measures, after,...   \n",
       "9   [But, Fischler, agreed, to, review, his, propo...   \n",
       "10  [Spanish, Farm, Minister, Loyola, de, Palacio,...   \n",
       "11                                                [.]   \n",
       "12  [Only, France, and, Britain, backed, Fischler,...   \n",
       "13  [The, EU, 's, scientific, veterinary, and, mul...   \n",
       "14  [Sheep, have, long, been, known, to, contract,...   \n",
       "15  [British, farmers, denied, on, Thursday, there...   \n",
       "16  [\", What, we, have, to, be, extremely, careful...   \n",
       "17  [Bonn, has, led, efforts, to, protect, public,...   \n",
       "18  [Germany, imported, 47,600, sheep, from, Brita...   \n",
       "19  [It, brought, in, 4,275, tonnes, of, British, ...   \n",
       "20  [Rare, Hendrix, song, draft, sells, for, almos...   \n",
       "21                               [LONDON, 1996-08-22]   \n",
       "22  [A, rare, early, handwritten, draft, of, a, so...   \n",
       "23  [A, Florida, restaurant, paid, 10,925, pounds,...   \n",
       "24  [At, the, end, of, a, January, 1967, concert, ...   \n",
       "25  [Buyers, also, snapped, up, 16, other, items, ...   \n",
       "26  [They, included, a, black, lacquer, and, mothe...   \n",
       "27  [The, guitarist, died, of, a, drugs, overdose,...   \n",
       "28  [China, says, Taiwan, spoils, atmosphere, for,...   \n",
       "29                              [BEIJING, 1996-08-22]   \n",
       "\n",
       "                                             ner_tags  \n",
       "0                         [3, 0, 7, 0, 0, 0, 7, 0, 0]  \n",
       "1                                              [1, 2]  \n",
       "2                                              [5, 0]  \n",
       "3   [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...  \n",
       "4   [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...  \n",
       "5   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7   [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, ...  \n",
       "8   [1, 0, 7, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, ...  \n",
       "9   [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, ...  \n",
       "10  [7, 0, 0, 1, 2, 2, 0, 0, 0, 1, 0, 0, 3, 0, 0, ...  \n",
       "11                                                [0]  \n",
       "12                        [0, 5, 0, 5, 0, 1, 0, 0, 0]  \n",
       "13  [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "14  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, ...  \n",
       "15  [7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "16  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "17  [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "18      [5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "19   [0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "20                     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "21                                             [5, 0]  \n",
       "22  [0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 2, 0, ...  \n",
       "23  [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "24  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 5, 0, ...  \n",
       "25  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "26  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "27               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "28                           [5, 0, 5, 0, 0, 0, 0, 0]  \n",
       "29                                             [5, 0]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.select(range(30)).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `document_id` and `sentence_id` columns contain integers. The former serves to identify which document the sentence belongs to, while the latter indicates the position of the sentence in the document. Internally, SpanMarker will include adjacent sentences originating from the same document as contextual information.  In the SpanMarker configuration, you can set `max_prev_context` and `max_next_context` to limit on the number of previous or next sentences to be included as context. By default, these are set to `None`, allowing the inclusion of as much context as is available until the maximum token length is reached. In practice, these settings are defined like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from span_marker import SpanMarkerModel\n",
    "\n",
    "# An example encoder and example labels\n",
    "model = SpanMarkerModel.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",  # Example encoder\n",
    "    labels=[  # Example labels\n",
    "        \"O\",\n",
    "        \"PER\",\n",
    "        \"LOC\",\n",
    "    ],\n",
    "    max_prev_context=2,\n",
    "    max_next_context=2,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using this dataset works equivalently as if the `document_id` and `sentence_id` columns did not exist. See the [Model Training](model_training.ipynb) tutorial for more information on how to do that. See also the [Trainer](https://tomaarsen.github.io/SpanMarkerNER/api/span_marker.trainer.html) documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "For inference, the inputs to `model.predict` must also contain `document_id` and `sentence_id` columns, alongside a `tokens` column that includes either string sentences or lists of tokens. Let's consider some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, this data is already split into sentences.\n",
    "# You can use various tools to do this, e.g. spaCy senter or NLTK sent_tokenize\n",
    "document_one = [\n",
    "    \"Cleopatra VII (70/69 BC - 10 August 30 BC) was Queen of the Ptolemaic Kingdom of Egypt from 51 to 30 BC, and its last active ruler.\",\n",
    "    \"A member of the Ptolemaic dynasty, she was a descendant of its founder Ptolemy I Soter, a Macedonian Greek general and companion of Alexander the Great.\",\n",
    "    \"After the death of Cleopatra, Egypt became a province of the Roman Empire, marking the end of the last Hellenistic state in the Mediterranean and of the age that had lasted since the reign of Alexander (336-323 BC).\",\n",
    "]\n",
    "\n",
    "document_two = [\n",
    "    \"The 35-year-old led his country to the 2022 World Cup title in Qatar last year, arguably the crowning triumph in one of the greatest football careers.\",\n",
    "    \"And on Thursday, Messi enjoyed another landmark moment by scoring his fastest ever goal.\",\n",
    "    \"Messi curled home an exquisite left-footed strike from the edge of the box just 79 seconds into Argentina's friendly against Australia in Beijing - the quickest of his professional career, per South American football's governing body, CONMEBOL.\",\n",
    "]\n",
    "\n",
    "document_three = [\n",
    "    \"UK firms could gain access to US green funding as part of plans to boost UK and US ties announced by Rishi Sunak and Joe Biden.\",\n",
    "    \"The pair unveiled the Atlantic Declaration, to strengthen economic ties between the two countries, at a White House press conference.\",\n",
    "    \"The PM said the agreement, which falls short of a full trade deal would bring benefits \\\"as quickly as possible\\\".\",\n",
    "    \"UK electric car firms may get access to US green tax credits and subsidies.\",\n",
    "    \"As the pair unveiled their partnership to bolster economic security, Mr Sunak said the UK-US relationship was an \\\"indispensable alliance\\\".\"\n",
    "]\n",
    "\n",
    "documents = [document_one, document_two, document_three]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to preprocess this dataset to generate the `document_id` and `sentence_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'document_id', 'sentence_id'],\n",
       "    num_rows: 11\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = {\n",
    "    \"tokens\": [],\n",
    "    \"document_id\": [],\n",
    "    \"sentence_id\": [],\n",
    "}\n",
    "for document_id, document in enumerate(documents):\n",
    "    for sentence_id, sentence in enumerate(document):\n",
    "        data_dict[\"document_id\"].append(document_id)\n",
    "        data_dict[\"sentence_id\"].append(sentence_id)\n",
    "        data_dict[\"tokens\"].append(sentence)\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>document_id</th>\n",
       "      <th>sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cleopatra VII (70/69 BC - 10 August 30 BC) was...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A member of the Ptolemaic dynasty, she was a d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After the death of Cleopatra, Egypt became a p...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 35-year-old led his country to the 2022 Wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And on Thursday, Messi enjoyed another landmar...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Messi curled home an exquisite left-footed str...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UK firms could gain access to US green funding...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The pair unveiled the Atlantic Declaration, to...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The PM said the agreement, which falls short o...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UK electric car firms may get access to US gre...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>As the pair unveiled their partnership to bols...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tokens  document_id  \\\n",
       "0   Cleopatra VII (70/69 BC - 10 August 30 BC) was...            0   \n",
       "1   A member of the Ptolemaic dynasty, she was a d...            0   \n",
       "2   After the death of Cleopatra, Egypt became a p...            0   \n",
       "3   The 35-year-old led his country to the 2022 Wo...            1   \n",
       "4   And on Thursday, Messi enjoyed another landmar...            1   \n",
       "5   Messi curled home an exquisite left-footed str...            1   \n",
       "6   UK firms could gain access to US green funding...            2   \n",
       "7   The pair unveiled the Atlantic Declaration, to...            2   \n",
       "8   The PM said the agreement, which falls short o...            2   \n",
       "9   UK electric car firms may get access to US gre...            2   \n",
       "10  As the pair unveiled their partnership to bols...            2   \n",
       "\n",
       "    sentence_id  \n",
       "0             0  \n",
       "1             1  \n",
       "2             2  \n",
       "3             0  \n",
       "4             1  \n",
       "5             2  \n",
       "6             0  \n",
       "7             1  \n",
       "8             2  \n",
       "9             3  \n",
       "10            4  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately pass this dataset to `SpanMarkerModel.predict`, and SpanMarker will under the hood add the document-level context for you. Note that the dataset does not need to be sorted. See also the [SpanMarkerModel.predict](https://tomaarsen.github.io/SpanMarkerNER/api/span_marker.modeling.html#span_marker.modeling.SpanMarkerModel.predict) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from span_marker import SpanMarkerModel\n",
    "\n",
    "model = SpanMarkerModel.from_pretrained(\"tomaarsen/span-marker-xlm-roberta-large-conll03-doc-context\").try_cuda()\n",
    "entities = model.predict(dataset)\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'span': 'Cleopatra VII',\n",
       "  'label': 'PER',\n",
       "  'score': 0.7116236686706543,\n",
       "  'char_start_index': 0,\n",
       "  'char_end_index': 13},\n",
       " {'span': 'BC',\n",
       "  'label': 'MISC',\n",
       "  'score': 0.9982840418815613,\n",
       "  'char_start_index': 21,\n",
       "  'char_end_index': 23},\n",
       " {'span': 'Ptolemaic Kingdom of Egypt',\n",
       "  'label': 'LOC',\n",
       "  'score': 0.6176416873931885,\n",
       "  'char_start_index': 60,\n",
       "  'char_end_index': 86}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the SpanMarker model returns a list of entity dictionaries for each sentence in the input dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "span-marker-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
